{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32121d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\lab8\\venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: asttokens==3.0.0 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: click==8.2.1 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 2)) (8.2.1)\n",
      "Requirement already satisfied: colorama==0.4.6 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: comm==0.2.2 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 4)) (0.2.2)\n",
      "Requirement already satisfied: debugpy==1.8.15 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 5)) (1.8.15)\n",
      "Requirement already satisfied: decorator==5.2.1 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 6)) (5.2.1)\n",
      "Requirement already satisfied: executing==2.2.0 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: ipykernel==6.30.0 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 8)) (6.30.0)\n",
      "Requirement already satisfied: ipython==9.4.0 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 9)) (9.4.0)\n",
      "Requirement already satisfied: ipython_pygments_lexers==1.1.1 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 10)) (1.1.1)\n",
      "Requirement already satisfied: jedi==0.19.2 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 11)) (0.19.2)\n",
      "Requirement already satisfied: joblib==1.5.1 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 12)) (1.5.1)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 13)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.8.1 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 14)) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 15)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 16)) (1.6.0)\n",
      "Requirement already satisfied: numpy==2.3.1 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 18)) (2.3.1)\n",
      "Requirement already satisfied: packaging==25.0 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 19)) (25.0)\n",
      "Requirement already satisfied: pandas==2.3.1 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 20)) (2.3.1)\n",
      "Requirement already satisfied: parso==0.8.4 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 21)) (0.8.4)\n",
      "Requirement already satisfied: platformdirs==4.3.8 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 22)) (4.3.8)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.51 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 23)) (3.0.51)\n",
      "Requirement already satisfied: psutil==7.0.0 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 24)) (7.0.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 25)) (0.2.3)\n",
      "Requirement already satisfied: Pygments==2.19.2 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 26)) (2.19.2)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 27)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2025.2 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 28)) (2025.2)\n",
      "Requirement already satisfied: pywin32==311 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 29)) (311)\n",
      "Requirement already satisfied: pyzmq==27.0.0 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 30)) (27.0.0)\n",
      "Requirement already satisfied: regex==2024.11.6 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 31)) (2024.11.6)\n",
      "Requirement already satisfied: six==1.17.0 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 32)) (1.17.0)\n",
      "Requirement already satisfied: stack-data==0.6.3 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 33)) (0.6.3)\n",
      "Requirement already satisfied: tornado==6.5.1 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 34)) (6.5.1)\n",
      "Requirement already satisfied: tqdm==4.67.1 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 35)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 36)) (5.14.3)\n",
      "Requirement already satisfied: tzdata==2025.2 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 37)) (2025.2)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in e:\\lab8\\venv\\lib\\site-packages (from -r requirements.txt (line 38)) (0.2.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c603e6",
   "metadata": {},
   "source": [
    "## NLP Pipeline + Probabilistic Language Models \n",
    "Building Unigram & Bigram Models from Real Text Data with NLTK\n",
    "\n",
    "- Construct an end-to-end NLP pipeline including tokenization and normalization.\n",
    "- Implement and analyze **Unigram** and **Bigram** language models.\n",
    "- Apply **Maximum Likelihood Estimation (MLE)** to compute sentence probabilities.\n",
    "- Explore how statistical models capture language patterns and dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fe484",
   "metadata": {},
   "source": [
    "## 1. Document Collection\n",
    "\n",
    "In this section, we download and load text data from the Gutenberg Corpus using the nltk library. This corpus contains a selection of classic texts such as works by Shakespeare, Jane Austen, and others.\n",
    "\n",
    "We begin by downloading the corpus.\n",
    "\n",
    "Then, we extract the raw text from each file in the corpus.\n",
    "\n",
    "Finally, we print the total number of documents available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d9d8b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 18\n",
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\shiru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "docs = [gutenberg.raw(file_id) for file_id in gutenberg.fileids()]\n",
    "print(f\"Total docs: {len(docs)}\")\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d9cdc9",
   "metadata": {},
   "source": [
    "## 2. Tokenizer Implementation\n",
    "\n",
    "Tokenization is the process of breaking raw text into individual tokens (words). For this task, we use a regular expression-based tokenizer to:\n",
    "- Convert text to lowercase.\n",
    "- Remove punctuation and symbols.\n",
    "- Split by word boundaries.\n",
    "\n",
    "This simple tokenizer is fast and suitable for basic information retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224fe930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens: ['emma', 'by', 'jane', 'austen', '1816', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Apply tokenizer to all documents\n",
    "tokenized_docs = [tokenize(doc) for doc in docs]\n",
    "print(\"Sample tokens:\", tokenized_docs[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd83c7",
   "metadata": {},
   "source": [
    "## 3. Normalization Pipeline\n",
    "\n",
    "Normalization prepares tokens for indexing and comparison. It includes:\n",
    "- Removing common stop words like \"the\", \"is\", \"in\".\n",
    "- Applying stemming using the Porter Stemmer, which reduces words to their base form (e.g., \"running\" → \"run\").\n",
    "\n",
    "This helps reduce vocabulary size and improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae16046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shiru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized sample: ['emma', 'jane', 'austen', '1816', 'volum', 'chapter', 'emma', 'woodhous', 'handsom', 'clever', 'rich', 'comfort', 'home', 'happi', 'disposit', 'seem', 'unit', 'best', 'bless', 'exist']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "# Apply normalization\n",
    "normalized_docs = [normalize(tokens) for tokens in tokenized_docs]\n",
    "print(\"Normalized sample:\", normalized_docs[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd855b",
   "metadata": {},
   "source": [
    "### Talking Points: Normalized Tokens Output ###\n",
    "Reduced Vocabulary Size\n",
    "\n",
    "Tokens are stemmed using PorterStemmer, so similar words (e.g., happiness, happy) are mapped to a common base (happi), reducing redundancy.\n",
    "\n",
    "Stopwords Removed\n",
    "\n",
    "Common English stopwords (e.g., was, and, the) have been successfully excluded, keeping only semantically rich words.\n",
    "\n",
    "Lowercased and Tokenized\n",
    "\n",
    "All tokens are in lowercase, ensuring consistency and avoiding duplication (e.g., Emma and emma treated the same).\n",
    "\n",
    "Domain-Specific Terms Retained\n",
    "\n",
    "Important character and domain-specific tokens like emma, jane, austen, and woodhous remain, which are useful for downstream tasks such as named entity recognition or topic modeling.\n",
    "\n",
    "Examples of Stemming\n",
    "\n",
    "Words like handsome → handsom, comfortable → comfort, happiness → happi demonstrate how morphological variants are reduced to their root form.\n",
    "\n",
    "Text Now Ready for Modeling\n",
    "\n",
    "The normalized output is a compact, semantically focused representation of the original text—ideal for building language models, calculating TF-IDF, or training classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc678bbf",
   "metadata": {},
   "source": [
    "## 4. Inverted Index Construction ###\n",
    "\n",
    "An inverted index maps each term to the list of documents it appears in. This allows efficient document retrieval given a search query.\n",
    "\n",
    "We build an inverted index using the normalized tokens from our document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d52947a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs containing 'scienc': [3, 8, 9, 10, 11, 12, 13, 17]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(docs):\n",
    "    index = defaultdict(set)\n",
    "    for doc_id, tokens in enumerate(docs):\n",
    "        for token in tokens:\n",
    "            index[token].add(doc_id)\n",
    "    return {term: list(doc_ids) for term, doc_ids in index.items()}\n",
    "\n",
    "inverted_index = build_inverted_index(normalized_docs)\n",
    "\n",
    "# Example term\n",
    "print(\"Docs containing 'scienc':\", inverted_index.get('scienc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856395c7",
   "metadata": {},
   "source": [
    "### Building an Inverted Index ###\n",
    "In this section, we construct an inverted index, which is a data structure commonly used in information retrieval systems (like search engines).\n",
    "\n",
    "What We Did:\n",
    "We defined the build_inverted_index function to map each unique term to the set of document IDs where that term appears.\n",
    "\n",
    "We applied this function to the normalized documents to build the index efficiently.\n",
    "\n",
    "We then retrieved a sample result: all documents containing the term 'scienc' (note the stemmed form of science)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702cb4f7",
   "metadata": {},
   "source": [
    "### Talking Points: Inverted Index Result ###\n",
    "Efficient Retrieval\n",
    "\n",
    "The inverted index enables fast lookup of which documents contain a given term, improving query efficiency in large corpora.\n",
    "\n",
    "Supports Boolean Queries\n",
    "\n",
    "This structure is foundational for building AND/OR/NOT query systems in search engines and recommender systems.\n",
    "\n",
    "Stemming Normalization\n",
    "\n",
    "The term 'scienc' is the stemmed form of science, meaning all morphological variants (e.g., scientific, sciences) map to this single key.\n",
    "\n",
    "Document Coverage Insight\n",
    "\n",
    "The output [3, 8, 9, 10, 11, 12, 13, 17] tells us which documents (by index) include the concept of science — useful for relevance ranking, search, or topic clustering.\n",
    "\n",
    "Scalability\n",
    "\n",
    "Using defaultdict(set) ensures that repeated terms across documents are aggregated efficiently, and conversion to a list ensures JSON-serializability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99382d",
   "metadata": {},
   "source": [
    "## Part 2 – Probabilistic Language Models\n",
    "\n",
    "###  Unigram Model\n",
    "We build a unigram probabilistic language model for one document using Laplace (add-one) smoothing. This model estimates the probability of a query based on the frequency of individual tokens in the document.\n",
    "\n",
    "Steps:\n",
    "Token counts are computed using Counter.\n",
    "\n",
    "Probabilities are calculated using:\n",
    "                             p(w) = count(w) + 1/total tokens + vocab size\n",
    "                            \n",
    " Log probabilities are used to avoid underflow in multiplication of small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49b170cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def unigram_model_with_laplace(doc_tokens, vocab):\n",
    "    total_tokens = len(doc_tokens)\n",
    "    vocab_size = len(vocab)\n",
    "    token_counts = Counter(doc_tokens)\n",
    "\n",
    "    def score(query_tokens):\n",
    "        prob_log = 0.0\n",
    "        for token in query_tokens:\n",
    "            count = token_counts.get(token, 0)\n",
    "            prob = (count + 1) / (total_tokens + vocab_size)\n",
    "            prob_log += math.log(prob)\n",
    "        return prob_log\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c759b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Score: -23.01797544445517\n"
     ]
    }
   ],
   "source": [
    "# Choose one document and build its model\n",
    "doc_id = 0\n",
    "doc_tokens = normalized_docs[doc_id]\n",
    "vocab = set(token for doc in normalized_docs for token in doc)\n",
    "\n",
    "model = unigram_model_with_laplace(doc_tokens, vocab)\n",
    "\n",
    "# Query scoring\n",
    "query = \"data science\".split()\n",
    "query_tokens = normalize(tokenize(\" \".join(query)))\n",
    "\n",
    "print(f\"Query Score: {model(query_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb65798",
   "metadata": {},
   "source": [
    "**Talking Points: Unigram Model Output**\n",
    "\n",
    "What the Score Means\n",
    "The score -23.01 is the log-probability of the query \"data science\" appearing in the selected document (doc_id = 0). Lower (more negative) scores indicate lower likelihood.\n",
    "\n",
    "Laplace Smoothing\n",
    "Ensures that even unseen words get a small non-zero probability, avoiding issues like log(0).\n",
    "\n",
    "Unigram Assumption\n",
    "Assumes independence between query words, which simplifies the model but ignores word order and context.\n",
    "\n",
    "Vocabulary-Aware Probability\n",
    "Probability is adjusted based on both the number of tokens in the document and the entire vocabulary seen across the corpus.\n",
    "\n",
    "Useful for Ranking Documents\n",
    "This score can be used to rank multiple documents by how likely they are to generate a given query—an essential technique in information retrieval systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb1f47",
   "metadata": {},
   "source": [
    "**Bigram Language Model (with Log-Probability Scoring)**\n",
    "\n",
    "In this step, we train a bigram language model to capture word-to-word transitions. Unlike the unigram model, which assumes each word is independent, the bigram model considers the previous word to predict the current word.\n",
    "\n",
    "What We Did:\n",
    "We built bigram and unigram frequency tables:\n",
    "\n",
    "bigram_counts[prev][curr]: Frequency of a word pair.\n",
    "\n",
    "unigram_counts[prev]: Frequency of the first word in each bigram.\n",
    "\n",
    "These counts will be used to calculate conditional probabilities:\n",
    "P(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i)}{\\text{count}(w_{i-1})}\n",
    "$$  \n",
    "Log-probabilities are used to avoid numerical underflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ee697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def train_bigram_model(tokens):\n",
    "    bigram_counts = defaultdict(Counter)\n",
    "    unigram_counts = Counter()\n",
    "\n",
    "    for i in range(1, len(tokens)):\n",
    "        prev_word = tokens[i - 1]\n",
    "        curr_word = tokens[i]\n",
    "        bigram_counts[prev_word][curr_word] += 1\n",
    "        unigram_counts[prev_word] += 1\n",
    "\n",
    "    return bigram_counts, unigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92a15776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_probability(query_tokens, bigram_counts, unigram_counts, vocab_size):\n",
    "    prob_log = 0.0\n",
    "\n",
    "    for i in range(1, len(query_tokens)):\n",
    "        prev = query_tokens[i - 1]\n",
    "        curr = query_tokens[i]\n",
    "        count_bigram = bigram_counts[prev].get(curr, 0)\n",
    "        count_unigram = unigram_counts.get(prev, 0)\n",
    "\n",
    "        # Laplace smoothing\n",
    "        prob = (count_bigram + 1) / (count_unigram + vocab_size)\n",
    "        prob_log += math.log(prob)\n",
    "\n",
    "    return prob_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48fa5757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram log-probability score: -20.337464559737946\n"
     ]
    }
   ],
   "source": [
    "# Train model on one document\n",
    "doc_id = 0\n",
    "doc_tokens = normalized_docs[doc_id]\n",
    "vocab = set(token for doc in normalized_docs for token in doc)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "bigram_counts, unigram_counts = train_bigram_model(doc_tokens)\n",
    "\n",
    "# Score a query\n",
    "query = \"data science is evolving\"\n",
    "query_tokens = normalize(tokenize(query))\n",
    "query_score = bigram_probability(query_tokens, bigram_counts, unigram_counts, vocab_size)\n",
    "\n",
    "print(f\"Bigram log-probability score: {query_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b363f",
   "metadata": {},
   "source": [
    "**Talking point**\n",
    "\n",
    "Contextual Modeling\n",
    "The bigram model captures local word dependencies, making it more powerful than the unigram model for modeling natural language.\n",
    "\n",
    "Interpretation of Score\n",
    "The log-probability score of -20.34 indicates how likely a sequence (e.g., a query) is under the trained model. A less negative score means a higher likelihood.\n",
    "\n",
    "Frequency-Based Estimation\n",
    "The model relies purely on observed frequencies. If a bigram hasn’t been seen in training, it would assign it a zero probability—hence the need for smoothing (e.g., add-one smoothing) in practice.\n",
    "\n",
    "Real-World Application\n",
    "Bigram models are commonly used in text prediction, speech recognition, and autocomplete systems.\n",
    "\n",
    "Next Step\n",
    "Combine this model with smoothing techniques (like Laplace or Kneser-Ney) for better generalization, especially in sparse data scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc1e8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample tokens from Emma by Jane Austen\n",
    "tokens = ['emma', 'by', 'jane', 'austen', '1816', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfaf057",
   "metadata": {},
   "source": [
    "### Laplace-smoothed bigram probability ###\n",
    "We implemented a Laplace-smoothed bigram language model to estimate the probability of a sentence using frequency counts of unigrams and bigrams.\n",
    "\n",
    "sentence = ['emma', 'was', 'clever']\n",
    "\n",
    "The model uses the following formula for Laplace-smoothed bigram probability:\n",
    "\n",
    "Where:\n",
    "\n",
    "    P(wi|wi-1)=Count(wi-1,wi)+1/Count(wi-1)+V\n",
    "\n",
    "\n",
    "V is the size of the vocabulary (number of unique words),\n",
    "\n",
    "Laplace smoothing ensures no zero probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dd3818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace-smoothed bigram probability for the sentence: 0.0030959752\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Step 1: Vocabulary\n",
    "vocab = set(tokens)\n",
    "V = len(vocab)\n",
    "\n",
    "# Step 2: Count unigrams and bigrams\n",
    "unigram_counts = defaultdict(int)\n",
    "bigram_counts = defaultdict(int)\n",
    "\n",
    "for i in range(len(tokens) - 1):\n",
    "    unigram_counts[tokens[i]] += 1\n",
    "    bigram = (tokens[i], tokens[i + 1])\n",
    "    bigram_counts[bigram] += 1\n",
    "\n",
    "# Add the last token to unigram count\n",
    "unigram_counts[tokens[-1]] += 1\n",
    "\n",
    "# Step 3: Define Laplace smoothed probability function\n",
    "def laplace_bigram_prob(w1, w2):\n",
    "    bigram = (w1, w2)\n",
    "    return (bigram_counts[bigram] + 1) / (unigram_counts[w1] + V)\n",
    "\n",
    "# Example sentence\n",
    "sentence = ['emma', 'was', 'clever']\n",
    "score = 1.0\n",
    "for i in range(1, len(sentence)):\n",
    "    prob = laplace_bigram_prob(sentence[i-1], sentence[i])\n",
    "    score *= prob\n",
    "\n",
    "print(f\"Laplace-smoothed bigram probability for the sentence: {score:.10f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec1de4",
   "metadata": {},
   "source": [
    "### Talking Points ###\n",
    "Why Laplace Smoothing?\n",
    "\n",
    "Adds 1 to each bigram count to avoid assigning zero probability to unseen word pairs.\n",
    "\n",
    "Especially useful when the dataset is small or sparse.\n",
    "\n",
    "What Does 0.0031 Mean?\n",
    "\n",
    "This is the estimated probability of the full sentence \"emma was clever\" under the model.\n",
    "\n",
    "It's a very low value, which is expected since multiplying probabilities < 1 quickly leads to small results.\n",
    "\n",
    "Interpretation\n",
    "\n",
    "A higher probability means the sentence is more natural or likely based on training data.\n",
    "\n",
    "Even this small value can be comparatively high for short sequences, depending on corpus size and vocabulary.\n",
    "\n",
    "Real-World Relevance\n",
    "\n",
    "Language models like this one are foundational in:\n",
    "\n",
    "Text prediction (e.g., keyboards)\n",
    "\n",
    "Speech recognition\n",
    "\n",
    "Machine translation\n",
    "\n",
    "Chatbots and autocomplete systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd4ebb1",
   "metadata": {},
   "source": [
    "### Log-probability score ###\n",
    "We implemented a bigram language model using Laplace smoothing (add-one smoothing) and computed the log-probability of a test sentence:\n",
    "sentence = ['emma', 'was', 'clever']\n",
    "\n",
    "\n",
    "### Key Steps ###\n",
    "Tokenization: We converted the raw text from Jane Austen's Emma into lowercase tokens.\n",
    "\n",
    "Bigram Count: We computed the frequency of all consecutive word pairs (bigrams).\n",
    "\n",
    "Laplace Smoothing: We applied add-one smoothing to handle unseen bigrams and avoid zero probabilities.\n",
    "\n",
    "Log Probability: log P(w1,w2,w3)=log P(w2|w1)+ log P(w3|w2)\n",
    "\n",
    "Laplace-smoothed bigram log-probability: -5.7777\n",
    "\n",
    "This means the natural log (ln) of the joint probability of the sentence \"emma was clever\" is -5.7777."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f14c31fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace-smoothed bigram log-probability: -5.7777\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Compute log-probability score \n",
    "log_score = 0.0\n",
    "for i in range(1, len(sentence)):\n",
    "    prob = laplace_bigram_prob(sentence[i-1], sentence[i])\n",
    "    log_score += math.log(prob)\n",
    "\n",
    "print(f\"Laplace-smoothed bigram log-probability: {log_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282bc8f4",
   "metadata": {},
   "source": [
    "### Talking Points ####\n",
    "Why Log-Probability?\n",
    "\n",
    "Probabilities get very small when multiplying many bigrams, so using log-probability prevents underflow.\n",
    "\n",
    "Log-space makes it easier to compare sentence likelihoods.\n",
    "\n",
    "Why Laplace Smoothing?\n",
    "\n",
    "Helps avoid zero probability for unseen word pairs.\n",
    "\n",
    "Ensures all bigrams, even unseen ones, have a non-zero value.\n",
    "\n",
    "How to Interpret -5.7777?\n",
    "\n",
    "This is a reasonably high probability for a 3-word sentence.\n",
    "\n",
    "A more negative value means lower likelihood under the model.\n",
    "\n",
    "You can convert this back to actual probability:\n",
    "    P=e −5.7777 ≈0.0031\n",
    "Model Use Cases\n",
    "\n",
    "Can rank sentences based on likelihood.\n",
    "\n",
    "Forms the foundation for spelling correction, text generation, and speech recognition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d9933",
   "metadata": {},
   "source": [
    "### Summary: Bigram Language Model\n",
    "\n",
    "The Bigram Model estimates the probability of a word given the one before it, allowing us to model word dependencies and improve accuracy over the Unigram Model. To handle sparsity and zero probabilities, we use Laplace smoothing.\n",
    "\n",
    "This model brings us closer to real-world text generation and ranking but can still struggle with long-range dependencies, which more advanced models like trigrams or neural LMs aim to solve.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
